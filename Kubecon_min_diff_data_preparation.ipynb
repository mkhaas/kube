{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Kubecon_min_diff_data_preparation.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mkhaas/kube/blob/master/Kubecon_min_diff_data_preparation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9NrhrZfkGlhv"
      },
      "source": [
        "##### Copyright 2020 The TensorFlow Authors."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "_44fi4-AGzT4"
      },
      "source": [
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jh2Sd3OgG17J"
      },
      "source": [
        "# MinDiff Data Preparation\n",
        "\n",
        "<div class=\"devsite-table-wrapper\"><table class=\"tfo-notebook-buttons\" align=\"left\">\n",
        "  <td><a target=\"_blank\" href=\"https://tensorflow.org/responsible_ai/model_remediation/min_diff/guide/min_diff_data_preparation.ipynb\">\n",
        "  <img src=\"https://www.tensorflow.org/images/tf_logo_32px.png\" />View on TensorFlow.org</a>\n",
        "</td>\n",
        "<td>\n",
        "  <a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/model-remediation/blob/master/docs/min_diff/guide/min_diff_data_preparation.ipynb\">\n",
        "  <img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\">Run in Google Colab</a>\n",
        "</td>\n",
        "<td>\n",
        "  <a target=\"_blank\" href=\"https://github.com/tensorflow/model-remediation/blob/master/docs/min_diff/guide/min_diff_data_preparation.ipynb\">\n",
        "  <img width=32px src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\">View source on GitHub</a>\n",
        "</td>\n",
        "<td>\n",
        "  <a target=\"_blank\" href=\"https://storage.googleapis.com/tensorflow_docs/model-remediation/docs/min_diff/guide/min_diff_data_preparation.ipynb\"><img src=\"https://www.tensorflow.org/images/download_logo_32px.png\" />Download notebook</a>\n",
        "</td>\n",
        "</table></div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GuAubYtgDiZ0"
      },
      "source": [
        "##Introduction\n",
        "\n",
        "When implementing MinDiff, you will need to make complex decisions as you choose and shape your input before passing it on to the model. These decisions will largely determine the behavior of MinDiff within your model.\n",
        "\n",
        "This guide will cover the technical aspects of this process, but will not discuss how to evaluate a model for fairness, or how to identify particular slices and metrics for evaluation. Please see the [Fairness Indicators guidance](https://www.tensorflow.org/responsible_ai/fairness_indicators/guide/guidance) for details on this.\n",
        "\n",
        "To demonstrate MinDiff, this guide uses the [UCI income dataset](https://archive.ics.uci.edu/ml/datasets/census+income). The model task is to predict whether an individual has an income exceeding $50k, based on various personal attributes. This guide assumes there is a problematic gap in the FNR (false negative rate) between `\"Male\"` and `\"Female\"` slices and the model owner (you) has decided to apply MinDiff to address the issue. For more information on the scenarios in which one might choose to apply MinDiff, see the [requirements page](https://www.tensorflow.org/responsible_ai/model_remediation/min_diff/guide/requirements).\n",
        "\n",
        "Note: We recognize the limitations of the categories used in the original dataset, and acknowledge that these terms do not encompass the full range of vocabulary used in describing gender. Further, we acknowledge that this task doesn’t represent a real-world use case, and is used only to demonstrate the technical details of the MinDiff library.\n",
        "\n",
        "MinDiff works by penalizing the difference in distribution scores between examples in two sets of data. This guide will demonstrate how to choose and construct these additional MinDiff sets as well as how to package everything together so that it can be passed to a model for training.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vd2pkMQZL4W2"
      },
      "source": [
        "##Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kmAHyZt9TErX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1188e689-3128-44d8-8491-3257beab5ab0"
      },
      "source": [
        "!pip install --upgrade tensorflow-model-remediation"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tensorflow-model-remediation\n",
            "  Downloading tensorflow_model_remediation-0.1.5-py3-none-any.whl (90 kB)\n",
            "\u001b[?25l\r\u001b[K     |███▋                            | 10 kB 21.4 MB/s eta 0:00:01\r\u001b[K     |███████▏                        | 20 kB 24.4 MB/s eta 0:00:01\r\u001b[K     |██████████▉                     | 30 kB 27.1 MB/s eta 0:00:01\r\u001b[K     |██████████████▍                 | 40 kB 28.6 MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 51 kB 24.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▋          | 61 kB 8.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▎      | 71 kB 8.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▉   | 81 kB 9.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 90 kB 4.0 MB/s \n",
            "\u001b[?25hCollecting mock\n",
            "  Downloading mock-4.0.3-py3-none-any.whl (28 kB)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from tensorflow-model-remediation) (0.3.4)\n",
            "Requirement already satisfied: tensorflow>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-model-remediation) (2.6.0)\n",
            "Requirement already satisfied: tensorflow-hub in /usr/local/lib/python3.7/dist-packages (from tensorflow-model-remediation) (0.12.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from tensorflow-model-remediation) (1.1.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from tensorflow-model-remediation) (1.15.0)\n",
            "Requirement already satisfied: wrapt~=1.12.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.0.0->tensorflow-model-remediation) (1.12.1)\n",
            "Requirement already satisfied: opt-einsum~=3.3.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.0.0->tensorflow-model-remediation) (3.3.0)\n",
            "Requirement already satisfied: clang~=5.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.0.0->tensorflow-model-remediation) (5.0)\n",
            "Requirement already satisfied: absl-py~=0.10 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.0.0->tensorflow-model-remediation) (0.12.0)\n",
            "Requirement already satisfied: typing-extensions~=3.7.4 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.0.0->tensorflow-model-remediation) (3.7.4.3)\n",
            "Requirement already satisfied: flatbuffers~=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.0.0->tensorflow-model-remediation) (1.12)\n",
            "Requirement already satisfied: keras~=2.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.0.0->tensorflow-model-remediation) (2.6.0)\n",
            "Requirement already satisfied: numpy~=1.19.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.0.0->tensorflow-model-remediation) (1.19.5)\n",
            "Requirement already satisfied: google-pasta~=0.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.0.0->tensorflow-model-remediation) (0.2.0)\n",
            "Requirement already satisfied: h5py~=3.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.0.0->tensorflow-model-remediation) (3.1.0)\n",
            "Requirement already satisfied: tensorflow-estimator~=2.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.0.0->tensorflow-model-remediation) (2.6.0)\n",
            "Requirement already satisfied: gast==0.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.0.0->tensorflow-model-remediation) (0.4.0)\n",
            "Requirement already satisfied: keras-preprocessing~=1.1.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.0.0->tensorflow-model-remediation) (1.1.2)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.37.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.0.0->tensorflow-model-remediation) (1.39.0)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.0.0->tensorflow-model-remediation) (3.17.3)\n",
            "Requirement already satisfied: termcolor~=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.0.0->tensorflow-model-remediation) (1.1.0)\n",
            "Requirement already satisfied: astunparse~=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.0.0->tensorflow-model-remediation) (1.6.3)\n",
            "Requirement already satisfied: wheel~=0.35 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.0.0->tensorflow-model-remediation) (0.37.0)\n",
            "Requirement already satisfied: tensorboard~=2.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.0.0->tensorflow-model-remediation) (2.6.0)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py~=3.1.0->tensorflow>=2.0.0->tensorflow-model-remediation) (1.5.2)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow>=2.0.0->tensorflow-model-remediation) (2.23.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow>=2.0.0->tensorflow-model-remediation) (0.4.5)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow>=2.0.0->tensorflow-model-remediation) (1.0.1)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow>=2.0.0->tensorflow-model-remediation) (57.4.0)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow>=2.0.0->tensorflow-model-remediation) (1.34.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow>=2.0.0->tensorflow-model-remediation) (1.8.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow>=2.0.0->tensorflow-model-remediation) (3.3.4)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow>=2.0.0->tensorflow-model-remediation) (0.6.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.6->tensorflow>=2.0.0->tensorflow-model-remediation) (0.2.8)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.6->tensorflow>=2.0.0->tensorflow-model-remediation) (4.7.2)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.6->tensorflow>=2.0.0->tensorflow-model-remediation) (4.2.2)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.6->tensorflow>=2.0.0->tensorflow-model-remediation) (1.3.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard~=2.6->tensorflow>=2.0.0->tensorflow-model-remediation) (4.6.4)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard~=2.6->tensorflow>=2.0.0->tensorflow-model-remediation) (0.4.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow>=2.0.0->tensorflow-model-remediation) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow>=2.0.0->tensorflow-model-remediation) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow>=2.0.0->tensorflow-model-remediation) (2021.5.30)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow>=2.0.0->tensorflow-model-remediation) (3.0.4)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.6->tensorflow>=2.0.0->tensorflow-model-remediation) (3.1.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->markdown>=2.6.8->tensorboard~=2.6->tensorflow>=2.0.0->tensorflow-model-remediation) (3.5.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->tensorflow-model-remediation) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->tensorflow-model-remediation) (2018.9)\n",
            "Installing collected packages: mock, tensorflow-model-remediation\n",
            "Successfully installed mock-4.0.3 tensorflow-model-remediation-0.1.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XRa49AkYS6n1"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow_model_remediation import min_diff\n",
        "from tensorflow_model_remediation.tools.tutorials_utils import uci as tutorials_utils"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R7rxz9wsv2tS"
      },
      "source": [
        "## Original Data\n",
        "\n",
        "For demonstration purposes and to reduce runtimes, this guide uses only a sample fraction of the UCI Income dataset. In a real production setting, the full dataset would be utilized."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WtOSyqTh7SVZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e2132adc-6a0d-41a0-bef6-8b74c19b855f"
      },
      "source": [
        "# Sampled at 0.3 for reduced runtimes.\n",
        "train = tutorials_utils.get_uci_data(split='train', sample=0.3)\n",
        "t1 = tutorials_utils.get_uci_data()\n",
        "print(t1.head)\n",
        "print(t1.tail)\n",
        "\n",
        "print(type(train))\n",
        "\n",
        "print(len(train), 'train examples')\n",
        "print(train.head)\n",
        "print(train.dtypes)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#####from google.colab import drive\n",
        "#####drive.mount('/drive')\n",
        "\n",
        "#Write the DataFrame to CSV file.\n",
        "#####with open('/drive/My Drive/XXXX/uci.csv', 'w') as f:\n",
        "#####  t1.to_csv(f)\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<bound method NDFrame.head of        age          workclass  ...  native-country  target\n",
            "0       39          State-gov  ...   United-States       0\n",
            "1       50   Self-emp-not-inc  ...   United-States       0\n",
            "2       38            Private  ...   United-States       0\n",
            "3       53            Private  ...   United-States       0\n",
            "4       28            Private  ...            Cuba       0\n",
            "...    ...                ...  ...             ...     ...\n",
            "32556   27            Private  ...   United-States       0\n",
            "32557   40            Private  ...   United-States       1\n",
            "32558   58            Private  ...   United-States       0\n",
            "32559   22            Private  ...   United-States       0\n",
            "32560   52       Self-emp-inc  ...   United-States       1\n",
            "\n",
            "[32561 rows x 13 columns]>\n",
            "<bound method NDFrame.tail of        age          workclass  ...  native-country  target\n",
            "0       39          State-gov  ...   United-States       0\n",
            "1       50   Self-emp-not-inc  ...   United-States       0\n",
            "2       38            Private  ...   United-States       0\n",
            "3       53            Private  ...   United-States       0\n",
            "4       28            Private  ...            Cuba       0\n",
            "...    ...                ...  ...             ...     ...\n",
            "32556   27            Private  ...   United-States       0\n",
            "32557   40            Private  ...   United-States       1\n",
            "32558   58            Private  ...   United-States       0\n",
            "32559   22            Private  ...   United-States       0\n",
            "32560   52       Self-emp-inc  ...   United-States       1\n",
            "\n",
            "[32561 rows x 13 columns]>\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "9768 train examples\n",
            "<bound method NDFrame.head of        age     workclass      education  ...  hours-per-week  native-country target\n",
            "29733   33       Private      Bachelors  ...              45   United-States      1\n",
            "235     59     State-gov        HS-grad  ...              40   United-States      0\n",
            "12172   27       Private      Bachelors  ...              40               ?      0\n",
            "5192    32       Private      Bachelors  ...              45   United-States      1\n",
            "32511   25     Local-gov      Bachelors  ...              40   United-States      0\n",
            "...    ...           ...            ...  ...             ...             ...    ...\n",
            "17273   57   Federal-gov        HS-grad  ...              35   United-States      0\n",
            "3052    30       Private   Some-college  ...              56   United-States      1\n",
            "1187    65       Private        HS-grad  ...              30   United-States      0\n",
            "29244   38       Private        HS-grad  ...              40   United-States      0\n",
            "16111   24   Federal-gov      Bachelors  ...              40   United-States      0\n",
            "\n",
            "[9768 rows x 13 columns]>\n",
            "age                int64\n",
            "workclass         object\n",
            "education         object\n",
            "education-num      int64\n",
            "marital-status    object\n",
            "occupation        object\n",
            "relationship      object\n",
            "sex               object\n",
            "capital-gain       int64\n",
            "capital-loss       int64\n",
            "hours-per-week     int64\n",
            "native-country    object\n",
            "target             int64\n",
            "dtype: object\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ELbff3-cZkfd"
      },
      "source": [
        "### Converting to `tf.data.Dataset`\n",
        "\n",
        "`MinDiffModel` requires that the input be a `tf.data.Dataset`. If you were using a different format of input prior to integrating MinDiff, you will have to convert your input data.\n",
        "\n",
        "Use `tf.data.Dataset.from_tensor_slices` to convert to `tf.data.Dataset`.\n",
        "\n",
        "```\n",
        "dataset = tf.data.Dataset.from_tensor_slices((x, y, weights))\n",
        "dataset.shuffle(...)  # Optional.\n",
        "dataset.batch(batch_size)\n",
        "```\n",
        "\n",
        "See [`Model.fit`](https://www.tensorflow.org/api_docs/python/tf/keras/Model#fit) documentation for details on equivalences between the two methods of input.\n",
        "\n",
        "In this guide, the input is downloaded as a Pandas DataFrame and therefore, needs this conversion."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XLEhhmxBjiG2"
      },
      "source": [
        "# Function to convert a DataFrame into a tf.data.Dataset.\n",
        "def df_to_dataset(dataframe, shuffle=True):\n",
        "  dataframe = dataframe.copy()\n",
        "  labels = dataframe.pop('target')\n",
        "  ds = tf.data.Dataset.from_tensor_slices((dict(dataframe), labels))\n",
        "  if shuffle:\n",
        "    ds = ds.shuffle(buffer_size=5000)  # Reasonable but arbitrary buffer_size.\n",
        "  return ds\n",
        "\n",
        "# Convert the train DataFrame into a Dataset.\n",
        "original_train_ds = df_to_dataset(train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XqiTRpVqGnGh"
      },
      "source": [
        "Note: The training dataset has not been batched yet but it will be later."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3hgcefRb84HT"
      },
      "source": [
        "## Creating MinDiff data\n",
        "\n",
        "During training, MinDiff will encourage the model to reduce differences in predictions between two additional datasets (which may include examples from the original dataset). The selection of these two datasets is the key decision which will determine the effect MinDiff has on the model.\n",
        "\n",
        "The two datasets should be picked such that the disparity in performance that you are trying to remediate is evident and well-represented. Since the goal is to reduce a gap in FNR between `\"Male\"` and `\"Female\"` slices, this means creating one dataset with only _positively_ labeled `\"Male\"` examples and another with only _positively_ labeled `\"Female\"` examples; these will be the MinDiff datasets.\n",
        "\n",
        "Note: The choice of using only _positively_ labeled examples is directly tied to the target metric. This guide is concerned with _false negatives_ which, by definition, are _positively_ labeled examples that were incorrectly classified.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uWaRK3DVBeB9"
      },
      "source": [
        "First, examine the data present."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q9BbPpNTHs86"
      },
      "source": [
        "female_pos = train[(train['sex'] == ' Female') & (train['target'] == 1)]\n",
        "male_pos = train[(train['sex'] == ' Male') & (train['target'] == 1)]\n",
        "print(len(female_pos), 'positively labeled female examples')\n",
        "print(len(male_pos), 'positively labeled male examples')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZTUFR5mlIN06"
      },
      "source": [
        "It is perfectly acceptable to create MinDiff datasets from subsets of the original dataset.\n",
        "\n",
        "While there aren't 5,000 or more positive `\"Male\"` examples as recommended in the [requirements guidance](https://www.tensorflow.org/responsible_ai/model_remediation/min_diff/guide/requirements#how_much_data_do_i_need), there are over 2,000 and it is reasonable to try with that many before collecting more data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0JtRx1iBIo7w"
      },
      "source": [
        "min_diff_male_ds = df_to_dataset(male_pos)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yn1dTuKRIyWK"
      },
      "source": [
        "Positive `\"Female\"` examples, however, are much scarcer at 385. This is probably too small for good performance and so will require pulling in additional examples.\n",
        "\n",
        "Note: Since this guide began by reducing the dataset via sampling, this problem (and the corresponding solution) may seem contrived. However, it serves as a good example of how to approach concerns about the size of your MinDiff datasets."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZzO6sOsDJsxA"
      },
      "source": [
        "full_uci_train = tutorials_utils.get_uci_data(split='train')\n",
        "augmented_female_pos = full_uci_train[((full_uci_train['sex'] == ' Female') &\n",
        "                                       (full_uci_train['target'] == 1))]\n",
        "print(len(augmented_female_pos), 'positively labeled female examples')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O6WLEgbaKD78"
      },
      "source": [
        "Using the full dataset has more than tripled the number of examples that can be used for MinDiff. It’s still low but it is enough to try as a first pass."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zB0UxWqfKhu3"
      },
      "source": [
        "min_diff_female_ds = df_to_dataset(augmented_female_pos)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XmO9mTyDSJYI"
      },
      "source": [
        "Both the MinDiff datasets are significantly smaller than the recommended 5,000 or more examples. While it is reasonable to attempt to apply MinDiff with the current data, you may need to consider collecting additional data if you observe poor performance or overfitting during training."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sq6nE2kCtVgP"
      },
      "source": [
        "### Using `tf.data.Dataset.filter`\n",
        "\n",
        "Alternatively, you can create the two MinDiff datasets directly from the converted original `Dataset`.\n",
        "\n",
        "Note: When using `.filter` it is recommended to use `.cache()` if the dataset can easily fit in memory for runtime performance. If it is too large to do so, consider storing your filtered datasets in your file system and reading them in."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "efFvtSCEubnm"
      },
      "source": [
        "# Male\n",
        "def male_predicate(x, y):\n",
        "  return tf.equal(x['sex'], b' Male') and tf.equal(y, 0)\n",
        "\n",
        "alternate_min_diff_male_ds = original_train_ds.filter(male_predicate).cache()\n",
        "\n",
        "# Female\n",
        "def female_predicate(x, y):\n",
        "  return tf.equal(x['sex'], b' Female') and tf.equal(y, 0)\n",
        "\n",
        "full_uci_train_ds = df_to_dataset(full_uci_train)\n",
        "alternate_min_diff_female_ds = full_uci_train_ds.filter(female_predicate).cache()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yf2WlD01MNS0"
      },
      "source": [
        "The resulting `alternate_min_diff_male_ds` and `alternate_min_diff_female_ds` will be equivalent in output to `min_diff_male_ds` and `min_diff_female_ds` respectively."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zmi3Tak3vqP4"
      },
      "source": [
        "## Constructing your Training Dataset\n",
        "\n",
        "As a final step, the three datasets (the two newly created ones and the original) need to be merged into a single dataset that can be passed to the model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uwM9ka-uxBGB"
      },
      "source": [
        "### Batching the datasets\n",
        "\n",
        "Before merging, the datasets need to batched.\n",
        "\n",
        "*   The original dataset can use the same batching that was used before integrating MinDiff.\n",
        "*   The MinDiff datasets do not need to have the same batch size as the original dataset. In all likelihood, a smaller one will perform just as well. While they don't even need to have the same batch size as each other, it is recommended to do so for best performance.\n",
        "\n",
        "While not strictly necessary, it is recommended to use `drop_remainder=True` for the two MinDiff datasets as this will ensure that they have consistent batch sizes.\n",
        "\n",
        "\n",
        "Warning: The 3 datasets must be batched **before** they are merged together. Failing to do so will likely result in unintended input shapes that will cause errors downstream."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RWoXWKNtwyYl"
      },
      "source": [
        "original_train_ds = original_train_ds.batch(128)  # Same as before MinDiff.\n",
        "\n",
        "# The MinDiff datasets can have a different batch_size from original_train_ds\n",
        "min_diff_female_ds = min_diff_female_ds.batch(32, drop_remainder=True)\n",
        "# Ideally we use the same batch size for both MinDiff datasets.\n",
        "min_diff_male_ds = min_diff_male_ds.batch(32, drop_remainder=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_trgfQuazSaE"
      },
      "source": [
        "### Packing the Datasets with `pack_min_diff_data`\n",
        "\n",
        "Once the datasets are prepared, pack them into a single dataset which will then be passed along to the model. A single batch from the resulting dataset will contain one batch from each of the three datasets you prepared previously.\n",
        "\n",
        "You can do this by using the provided `utils` function in the `tensorflow_model_remediation` package:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m3-WDjO_z-cR"
      },
      "source": [
        "train_with_min_diff_ds = min_diff.keras.utils.pack_min_diff_data(\n",
        "    original_dataset=original_train_ds,\n",
        "    sensitive_group_dataset=min_diff_female_ds,\n",
        "    nonsensitive_group_dataset=min_diff_male_ds)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A-Z5Qjkm0xjM"
      },
      "source": [
        "And that's it! You will be able to use other `util` functions in the package to unpack individual batches if needed."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k8cCF0Sp1Dp6"
      },
      "source": [
        "for inputs, original_labels in train_with_min_diff_ds.take(1):\n",
        "  # Unpacking min_diff_data\n",
        "  min_diff_data = min_diff.keras.utils.unpack_min_diff_data(inputs)\n",
        "  min_diff_examples, min_diff_membership = min_diff_data\n",
        "  # Unpacking original data\n",
        "  original_inputs = min_diff.keras.utils.unpack_original_inputs(inputs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MkuIAPh5af20"
      },
      "source": [
        "With your newly formed data, you are now ready to apply MinDiff in your model! To learn how this is done, please take a look at the other guides starting with [Integrating MinDiff with MinDiffModel](./integrating_min_diff_with_min_diff_model)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4i-SwiEe3zji"
      },
      "source": [
        "### Using a Custom Packing Format (optional)\n",
        "\n",
        "You may decide to pack the three datasets together in whatever way you choose. The only requirement is that you will need to ensure the model knows how to interpret the data. The default implementation of `MinDiffModel` assumes that the data was packed using `min_diff.keras.utils.pack_min_diff_data`.\n",
        "\n",
        "One easy way to format your input as you want is to transform the data as a final step after you have used `min_diff.keras.utils.pack_min_diff_data`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eihOp-6E48Tk"
      },
      "source": [
        "# Reformat input to be a dict.\n",
        "def _reformat_input(inputs, original_labels):\n",
        "  unpacked_min_diff_data = min_diff.keras.utils.unpack_min_diff_data(inputs)\n",
        "  unpacked_original_inputs = min_diff.keras.utils.unpack_original_inputs(inputs)\n",
        "\n",
        "  return {\n",
        "      'min_diff_data': unpacked_min_diff_data,\n",
        "      'original_data': (unpacked_original_inputs, original_labels)}\n",
        "\n",
        "customized_train_with_min_diff_ds = train_with_min_diff_ds.map(_reformat_input)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vozEV0ST60sN"
      },
      "source": [
        "Your model will need to know how to read this customized input as detailed in the [Customizing MinDiffModel guide](./customizing_min_diff_model#customizing_default_behaviors_of_mindiffmodel)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zgWyP9Om66SF"
      },
      "source": [
        "for batch in customized_train_with_min_diff_ds.take(1):\n",
        "  # Customized unpacking of min_diff_data\n",
        "  min_diff_data = batch['min_diff_data']\n",
        "  # Customized unpacking of original_data\n",
        "  original_data = batch['original_data']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XXJ_ShDEmMAy"
      },
      "source": [
        "## Additional Resources\n",
        "\n",
        "*   For an in depth discussion on fairness evaluation see the [Fairness Indicators guidance](https://www.tensorflow.org/responsible_ai/fairness_indicators/guide/guidance)\n",
        "*   For general information on Remediation and MinDiff, see the [remediation overview](https://www.tensorflow.org/responsible_ai/model_remediation).\n",
        "*    For details on requirements surrounding MinDiff see [this guide](https://www.tensorflow.org/responsible_ai/model_remediation/min_diff/guide/requirements).\n",
        "*   To see an end-to-end tutorial on using MinDiff in Keras, see [this tutorial](https://www.tensorflow.org/responsible_ai/model_remediation/min_diff/tutorials/min_diff_keras)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7xbNdDoIJLBt"
      },
      "source": [
        "## Utility Functions for other Guides\n",
        "\n",
        "This guide outlines the process and decision making that you can follow whenever applying MinDiff. The rest of the guides build off this framework. To make this easier, logic found in this guide has been factored out into helper functions:\n",
        "\n",
        "*   `get_uci_data`: This function is already used in this guide. It returns a `DataFrame` containing the UCI income data from the indicated split sampled at whatever rate is indicated (100% if unspecified).\n",
        "*   `df_to_dataset`: This function converts a `DataFrame` into a `tf.data.Dataset` as detailed in this guide with the added functionality of being able to pass the batch_size as a parameter.\n",
        "*   `get_uci_with_min_diff_dataset`: This function returns a `tf.data.Dataset` containing both the original data and the MinDiff data packed together using the Model Remediation Library util functions as described in this guide.\n",
        "\n",
        "Warning: These utility functions are **not** part of the official `tensorflow-model-remediation` package API and are subject to change at any time.\n",
        "\n",
        "The rest of the guides will build off of these to show how to use other parts of the library."
      ]
    }
  ]
}